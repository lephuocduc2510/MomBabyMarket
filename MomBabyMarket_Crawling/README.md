# MomBabyMarket CrawlerTool crawl chuyÃªn nghiá»‡p cho dá»± Ã¡n MomBabyMarket case study sá»­ dá»¥ng JavaScript vÃ  Docker.## ğŸš€ TÃ­nh nÄƒng- **Multi-platform**: Facebook, Instagram, E-commerce websites- **Docker containerized**: Deploy chuyÃªn nghiá»‡p vá»›i Docker- **Service Pattern**: Kiáº¿n trÃºc module rÃµ rÃ ng, dá»… báº£o trÃ¬- **Image downloading**: Tá»± Ä‘á»™ng download vÃ  tá»‘i Æ°u áº£nh- **Comprehensive logging**: Há»‡ thá»‘ng log chi tiáº¿t- **Error handling**: Xá»­ lÃ½ lá»—i vÃ  retry mechanism## ğŸ“‹ YÃªu cáº§u- Node.js 18+ (náº¿u cháº¡y local)- Docker & Docker Compose (khuyáº¿n nghá»‹)- 4GB+ RAM cho browser instances## ğŸ› ï¸ CÃ i Ä‘áº·t & Cháº¡y### Option 1: Docker (Khuyáº¿n nghá»‹)```bash# Navigate to projectcd MomBabyMarket_Crawling# CÃ i Ä‘áº·t dependenciesnpm install# Build vÃ  cháº¡y vá»›i Dockernpm run docker:buildnpm run docker:run# Xem logsnpm run docker:logs# Dá»«ng containersnpm run docker:stop```### Option 2: Local Development```bash# CÃ i Ä‘áº·t dependenciesnpm install# Táº¡o thÆ° má»¥c outputmkdir -p data/images logs# Cháº¡y development modenpm run dev# Hoáº·c cháº¡y productionnpm start```## âš™ï¸ Cáº¥u hÃ¬nh### Environment Variables (.env)```env# Crawler SettingsNODE_ENV=developmentHEADLESS=trueCRAWL_DELAY=3000MAX_RETRIES=3TIMEOUT=30000# Browser SettingsUSER_AGENT=Mozilla/5.0 (Windows NT 10.0; Win64; x64)...VIEWPORT_WIDTH=1366VIEWPORT_HEIGHT=768# Output SettingsOUTPUT_DIR=./dataIMAGES_DIR=./data/images# LoggingLOG_LEVEL=info```### Targets ConfigurationFile `src/config/targets.js` chá»©a 32 targets:- 16 Facebook pages- 6 Instagram accounts  - 10 E-commerce websites## ğŸ“Š Cáº¥u trÃºc Output```data/â”œâ”€â”€ crawled_data.json          # Dá»¯ liá»‡u chÃ­nhâ”œâ”€â”€ crawl_stats.json           # Thá»‘ng kÃª crawlâ”œâ”€â”€ summary_report.json        # BÃ¡o cÃ¡o tá»•ng há»£pâ””â”€â”€ images/                    # áº¢nh Ä‘Ã£ download    â”œâ”€â”€ facebook/    â”œâ”€â”€ instagram/    â””â”€â”€ website/```### Format dá»¯ liá»‡u JSON```json{  "title": "TiÃªu Ä‘á» bÃ i viáº¿t hoáº·c sáº£n pháº©m",  "content": "Ná»™i dung bÃ i viáº¿t (náº¿u cÃ³)",  "imageUrl": "URL áº£nh gá»‘c",  "localImagePath": "data/images/facebook/image.jpg",  "articleUrl": "https://facebook.com/post/123",  "publishedAt": "2025-01-15T10:30:00.000Z",  "source": "https://www.facebook.com/mamako.mgl/",  "platform": "facebook",  "crawledAt": "2025-01-15T12:00:00.000Z"}```## ğŸ—ï¸ Kiáº¿n trÃºc Service Pattern```src/â”œâ”€â”€ config/â”‚   â””â”€â”€ targets.js             # Cáº¥u hÃ¬nh 32 targetsâ”œâ”€â”€ services/â”‚   â”œâ”€â”€ browserService.js      # Quáº£n lÃ½ Puppeteer browserâ”‚   â”œâ”€â”€ imageService.js        # Download & optimize imagesâ”‚   â”œâ”€â”€ facebookCrawlerService.jsâ”‚   â”œâ”€â”€ instagramCrawlerService.jsâ”‚   â”œâ”€â”€ websiteCrawlerService.jsâ”‚   â”œâ”€â”€ fileService.js         # Save results to filesâ”‚   â””â”€â”€ crawlerService.js      # Main orchestratorâ”œâ”€â”€ utils/â”‚   â””â”€â”€ logger.js              # Winston loggingâ””â”€â”€ index.js                   # Entry point```## ğŸ¯ Platforms Ä‘Æ°á»£c há»— trá»£### Facebook Pages- Extract posts vá»›i titles, images, links- Dynamic content loading- Automatic scrolling### Instagram Accounts  - Crawl individual posts tá»« profile- Extract captions vÃ  images- Support posts vÃ  reels### E-commerce Websites- **emonos.mn**: Product listings- **jivh-hurgelt.mn**: Category-based crawling- **babyworld.mn**: Product information- **nomin.mn**: Articles vÃ  products## ğŸ”§ Advanced Usage### Custom Settings```bash# Crawl vá»›i headful mode (show browser)HEADLESS=false npm run dev# TÄƒng delay giá»¯a requestsCRAWL_DELAY=5000 npm start# Debug modeLOG_LEVEL=debug npm run dev```### Docker Commands```bash# Build custom imagedocker build -t my-crawler .# Run vá»›i custom settingsdocker run -e HEADLESS=false -v $(pwd)/data:/app/data my-crawler# Check logsdocker logs mombabymarket-crawler```## ğŸ“ˆ Performance & Monitoring- **Error handling**: Automatic retry vá»›i exponential backoff- **Rate limiting**: Built-in delays giá»¯a requests- **Memory management**: Proper browser cleanup- **Resource optimization**: Block CSS/fonts Ä‘á»ƒ tÄƒng tá»‘c## ğŸš€ Cháº¡y Crawler```bash# Development modenpm run dev# Production mode  npm start# Docker modenpm run docker:run```## ğŸ“ Output FilesSau khi cháº¡y, check cÃ¡c files:1. **`data/crawled_data.json`** - Dá»¯ liá»‡u chÃ­nh Ä‘á»ƒ import vÃ o MongoDB2. **`data/images/`** - áº¢nh Ä‘Ã£ download theo platform3. **`data/crawl_stats.json`** - Thá»‘ng kÃª performance4. **`logs/crawler.log`** - Log files chi tiáº¿t## ğŸ”„ Workflow tiáº¿p theo1. **Review results**: Check `data/crawled_data.json`2. **Validate images**: Verify áº£nh trong `data/images/`3. **Import to MongoDB**: Sá»­ dá»¥ng data cho Pháº§n 24. **API integration**: Connect tá»›i backend (Pháº§n 3)## ğŸ“„ LicenseMIT License---**Hekate Team** - Building the future of AI-powered market intelligence